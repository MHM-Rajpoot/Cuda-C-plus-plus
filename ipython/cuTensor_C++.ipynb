{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SetUp"
      ],
      "metadata": {
        "id": "_5uxgMB_bEuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/*release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWTfunEutz12",
        "outputId": "f42b8096-6159-4d2a-f582-5d41d6a75de1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DISTRIB_ID=Ubuntu\n",
            "DISTRIB_RELEASE=22.04\n",
            "DISTRIB_CODENAME=jammy\n",
            "DISTRIB_DESCRIPTION=\"Ubuntu 22.04.4 LTS\"\n",
            "PRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /usr/local -name 'libcublasLt.so.*'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqZIXDHZ3SC_",
        "outputId": "ff06919b-ffbd-412a-e104-9978561474ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nvidia/cublas/lib/libcublasLt.so.12\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/libcublasLt.so.12.5.3.2\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/libcublasLt.so.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check CUDA version (important for downloading correct cuTENSOR build)\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MygzxfTtZah",
        "outputId": "64ec5316-f63e-4dbe-a5f1-2fee54869484"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w4JbMhkxtLtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95fbdeb-482f-44fc-a1cd-d634cccda97d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-15 09:48:47--  https://developer.download.nvidia.com/compute/cutensor/2.0.0/local_installers/cutensor-local-repo-ubuntu2004-2.0.0_1.0-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.59.88.2, 23.59.88.16\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.59.88.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 534317950 (510M) [application/x-deb]\n",
            "Saving to: ‘cutensor-local-repo-ubuntu2004-2.0.0_1.0-1_amd64.deb’\n",
            "\n",
            "cutensor-local-repo 100%[===================>] 509.56M   167MB/s    in 3.1s    \n",
            "\n",
            "2025-09-15 09:48:50 (167 MB/s) - ‘cutensor-local-repo-ubuntu2004-2.0.0_1.0-1_amd64.deb’ saved [534317950/534317950]\n",
            "\n",
            "Selecting previously unselected package cutensor-local-repo-ubuntu2004-2.0.0.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack cutensor-local-repo-ubuntu2004-2.0.0_1.0-1_amd64.deb ...\n",
            "Unpacking cutensor-local-repo-ubuntu2004-2.0.0 (1.0-1) ...\n",
            "Setting up cutensor-local-repo-ubuntu2004-2.0.0 (1.0-1) ...\n",
            "\n",
            "The public cutensor-local-repo-ubuntu2004-2.0.0 GPG key does not appear to be installed.\n",
            "To install the key, run this command:\n",
            "sudo cp /var/cutensor-local-repo-ubuntu2004-2.0.0/cutensor-local-658BF15C-keyring.gpg /usr/share/keyrings/\n",
            "\n",
            "Get:1 file:/var/cutensor-local-repo-ubuntu2004-2.0.0  InRelease [1,572 B]\n",
            "Get:1 file:/var/cutensor-local-repo-ubuntu2004-2.0.0  InRelease [1,572 B]\n",
            "Get:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 file:/var/cutensor-local-repo-ubuntu2004-2.0.0  Packages [869 B]\n",
            "Get:7 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,006 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,794 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,264 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:19 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,627 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [43.2 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [88.8 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Fetched 30.0 MB in 3s (11.5 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libcutensor-dev libcutensor-doc libcutensor2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 720 MB of archives.\n",
            "After this operation, 2,329 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcutensor2 2.2.0.0-1 [289 MB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcutensor-dev 2.2.0.0-1 [431 MB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcutensor-doc 2.2.0.0-1 [8,816 B]\n",
            "Fetched 720 MB in 19s (38.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libcutensor2.\n",
            "(Reading database ... 126390 files and directories currently installed.)\n",
            "Preparing to unpack .../libcutensor2_2.2.0.0-1_amd64.deb ...\n",
            "Unpacking libcutensor2 (2.2.0.0-1) ...\n",
            "Selecting previously unselected package libcutensor-dev.\n",
            "Preparing to unpack .../libcutensor-dev_2.2.0.0-1_amd64.deb ...\n",
            "Unpacking libcutensor-dev (2.2.0.0-1) ...\n",
            "Selecting previously unselected package libcutensor-doc.\n",
            "Preparing to unpack .../libcutensor-doc_2.2.0.0-1_amd64.deb ...\n",
            "Unpacking libcutensor-doc (2.2.0.0-1) ...\n",
            "Setting up libcutensor2 (2.2.0.0-1) ...\n",
            "update-alternatives: using /usr/lib/x86_64-linux-gnu/libcutensor/12/libcutensor.so.2.2.0 to provide /usr/lib/x86_64-linux-gnu/libcutensor.so.2.2.0 (libcutensor.so.2.2.0) in auto mode\n",
            "update-alternatives: using /usr/lib/x86_64-linux-gnu/libcutensor.so.2.2.0 to provide /usr/lib/x86_64-linux-gnu/libcutensor.so.2 (libcutensor.so.2) in auto mode\n",
            "update-alternatives: using /usr/lib/x86_64-linux-gnu/libcutensor/12/libcutensorMg.so.2.2.0 to provide /usr/lib/x86_64-linux-gnu/libcutensorMg.so.2.2.0 (libcutensorMg.so.2.2.0) in auto mode\n",
            "update-alternatives: using /usr/lib/x86_64-linux-gnu/libcutensorMg.so.2.2.0 to provide /usr/lib/x86_64-linux-gnu/libcutensorMg.so.2 (libcutensorMg.so.2) in auto mode\n",
            "Setting up libcutensor-dev (2.2.0.0-1) ...\n",
            "update-alternatives: using /usr/lib/x86_64-linux-gnu/libcutensor/12/libcutensor_static.a to provide /usr/lib/x86_64-linux-gnu/libcutensor_static.a (libcutensor_static.a) in auto mode\n",
            "update-alternatives: using /usr/lib/x86_64-linux-gnu/libcutensor/12/libcutensorMg_static.a to provide /usr/lib/x86_64-linux-gnu/libcutensorMg_static.a (libcutensorMg_static.a) in auto mode\n",
            "Setting up libcutensor-doc (2.2.0.0-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example: download cuTENSOR 2.0 for Linux x86_64 CUDA 11.x\n",
        "# (Update the link depending on CUDA version you saw above)\n",
        "!wget https://developer.download.nvidia.com/compute/cutensor/2.0.0/local_installers/cutensor-local-repo-ubuntu2004-2.0.0_1.0-1_amd64.deb\n",
        "\n",
        "# Extract\n",
        "!sudo dpkg -i cutensor-local-repo-ubuntu2004-2.0.0_1.0-1_amd64.deb\n",
        "!sudo cp /var/cutensor-local-repo-ubuntu2004-2.0.0/cutensor-*-keyring.gpg /usr/share/keyrings/\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install libcutensor2 libcutensor-dev libcutensor-doc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dpkg -l | grep cutensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n_KmeinyMyj",
        "outputId": "f2d52690-204f-42ed-c770-7dbd6ec1df1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ii  cutensor-local-repo-ubuntu2004-2.0.0   1.0-1                                   amd64        cutensor-local repository configuration files\n",
            "ii  libcutensor-dev                        2.2.0.0-1                               amd64        cuTensor native dev links, headers\n",
            "ii  libcutensor-doc                        2.2.0.0-1                               amd64        cuTensor documentation\n",
            "ii  libcutensor2                           2.2.0.0-1                               amd64        cuTensor native runtime libraries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/include/cutensor*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SKve3uXySn8",
        "outputId": "2efaaf0c-28de-4517-828d-160716a8dde1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/include/cutensor.h  /usr/include/cutensorMg.h\n",
            "\n",
            "/usr/include/cutensor:\n",
            "types.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 00_test_cutensor.cu\n",
        "#include <cutensor.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "int main() {\n",
        "    cutensorHandle_t handle;\n",
        "    cutensorStatus_t status = cutensorCreate(&handle);\n",
        "\n",
        "    if (status == CUTENSOR_STATUS_SUCCESS) {\n",
        "        std::cout << \"✅ cuTENSOR initialized successfully!\" << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"❌ cuTENSOR init failed: \" << status << std::endl;\n",
        "    }\n",
        "\n",
        "    cutensorDestroy(handle); // cleanup\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmP9O2yZtafV",
        "outputId": "617eb219-9659-4a24-f2c1-ff93cd1bb358"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 00_test_cutensor.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc test_cutensor.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcublasLt -lcublas -o test_cutensor\n",
        "\n",
        "!./test_cutensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7WAM1MQxvdy",
        "outputId": "9ba70fc3-8bb0-4180-9239-acf4daec7b50"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ktest_cutensor.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./test_cutensor: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /usr -name \"libcublasLt.so*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH9DnZCl1RGs",
        "outputId": "7290cfed-47ba-424d-c018-5600dbc68fea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nvidia/cublas/lib/libcublasLt.so.12\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/libcublasLt.so\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/stubs/libcublasLt.so\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/libcublasLt.so.12.5.3.2\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/libcublasLt.so.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BASIC LINEAR ALGEBRA"
      ],
      "metadata": {
        "id": "RtxCVsyZbij4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scalars, Vectors, Matrices, and Tensors"
      ],
      "metadata": {
        "id": "cXGQ3O1ycpA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 01_cutensor_basics.cu\n",
        "#include <cutensor.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "\n",
        "int main() {\n",
        "    // Initialize cuTENSOR\n",
        "    cutensorHandle_t handle;\n",
        "    cutensorStatus_t status = cutensorCreate(&handle);\n",
        "\n",
        "    if (status == CUTENSOR_STATUS_SUCCESS) {\n",
        "        std::cout << \"✅ cuTENSOR initialized successfully!\" << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"❌ cuTENSOR init failed: \" << status << std::endl;\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // --- 1. Scalar (just a single number) ---\n",
        "    float scalar = 3.14f;\n",
        "    std::cout << \"Scalar example: \" << scalar << std::endl;\n",
        "\n",
        "    // --- 2. Vector (1D array) ---\n",
        "    int n = 4;\n",
        "    std::vector<float> h_vector = {1.0, 2.0, 3.0, 4.0};\n",
        "    float* d_vector;\n",
        "    cudaMalloc(&d_vector, n * sizeof(float));\n",
        "    cudaMemcpy(d_vector, h_vector.data(), n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    std::cout << \"Vector example: [1, 2, 3, 4]\" << std::endl;\n",
        "\n",
        "    // --- 3. Matrix (2D array) ---\n",
        "    int rows = 2, cols = 3;\n",
        "    std::vector<float> h_matrix = {\n",
        "        1, 2, 3,\n",
        "        4, 5, 6\n",
        "    };\n",
        "    float* d_matrix;\n",
        "    cudaMalloc(&d_matrix, rows * cols * sizeof(float));\n",
        "    cudaMemcpy(d_matrix, h_matrix.data(), rows * cols * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    std::cout << \"Matrix example (2x3): [[1,2,3],[4,5,6]]\" << std::endl;\n",
        "\n",
        "    // --- 4. Tensor (3D array, e.g., RGB image 2x2x3) ---\n",
        "    int dimX = 2, dimY = 2, dimC = 3;\n",
        "    std::vector<float> h_tensor = {\n",
        "        // pixel (0,0)\n",
        "        255, 0, 0,   // Red\n",
        "        // pixel (0,1)\n",
        "        0, 255, 0,   // Green\n",
        "        // pixel (1,0)\n",
        "        0, 0, 255,   // Blue\n",
        "        // pixel (1,1)\n",
        "        255, 255, 0  // Yellow\n",
        "    };\n",
        "    float* d_tensor;\n",
        "    cudaMalloc(&d_tensor, dimX * dimY * dimC * sizeof(float));\n",
        "    cudaMemcpy(d_tensor, h_tensor.data(), dimX * dimY * dimC * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    std::cout << \"Tensor example (2x2x3 RGB image)\" << std::endl;\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_matrix);\n",
        "    cudaFree(d_tensor);\n",
        "    cutensorDestroy(handle);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hdQ7gjCbmk4",
        "outputId": "75049b5b-9082-414e-d290-9881f102282b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 01_cutensor_basics.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc cutensor_basics.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcublasLt -lcublas -o cutensor_basics\n",
        "\n",
        "!./cutensor_basics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Olpj3lc7cRdY",
        "outputId": "8e018bc9-8b09-4aba-b9b0-27cad7cb00ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kcutensor_basics.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./cutensor_basics: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector Operations"
      ],
      "metadata": {
        "id": "o_HvK6jNc7Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 02_vector_ops.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "\n",
        "// --- Helper: print vector ---\n",
        "void printVector(const std::vector<float>& v, const std::string& name) {\n",
        "    std::cout << name << \" = [ \";\n",
        "    for (auto x : v) std::cout << x << \" \";\n",
        "    std::cout << \"]\" << std::endl;\n",
        "}\n",
        "\n",
        "// --- Vector Addition ---\n",
        "std::vector<float> add(const std::vector<float>& a, const std::vector<float>& b) {\n",
        "    std::vector<float> result(a.size());\n",
        "    for (size_t i = 0; i < a.size(); i++) result[i] = a[i] + b[i];\n",
        "    return result;\n",
        "}\n",
        "\n",
        "// --- Vector Subtraction ---\n",
        "std::vector<float> subtract(const std::vector<float>& a, const std::vector<float>& b) {\n",
        "    std::vector<float> result(a.size());\n",
        "    for (size_t i = 0; i < a.size(); i++) result[i] = a[i] - b[i];\n",
        "    return result;\n",
        "}\n",
        "\n",
        "// --- Scalar Multiplication ---\n",
        "std::vector<float> scalarMultiply(const std::vector<float>& a, float s) {\n",
        "    std::vector<float> result(a.size());\n",
        "    for (size_t i = 0; i < a.size(); i++) result[i] = a[i] * s;\n",
        "    return result;\n",
        "}\n",
        "\n",
        "// --- Dot Product ---\n",
        "float dot(const std::vector<float>& a, const std::vector<float>& b) {\n",
        "    float result = 0.0f;\n",
        "    for (size_t i = 0; i < a.size(); i++) result += a[i] * b[i];\n",
        "    return result;\n",
        "}\n",
        "\n",
        "// --- Cross Product (3D only) ---\n",
        "std::vector<float> cross(const std::vector<float>& a, const std::vector<float>& b) {\n",
        "    return {\n",
        "        a[1]*b[2] - a[2]*b[1],\n",
        "        a[2]*b[0] - a[0]*b[2],\n",
        "        a[0]*b[1] - a[1]*b[0]\n",
        "    };\n",
        "}\n",
        "\n",
        "// --- L1 Norm (Manhattan) ---\n",
        "float l1Norm(const std::vector<float>& a) {\n",
        "    float sum = 0.0f;\n",
        "    for (auto x : a) sum += std::fabs(x);\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "// --- L2 Norm (Euclidean) ---\n",
        "float l2Norm(const std::vector<float>& a) {\n",
        "    float sum = 0.0f;\n",
        "    for (auto x : a) sum += x * x;\n",
        "    return std::sqrt(sum);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Example vectors\n",
        "    std::vector<float> v1 = {1, 2, 3};\n",
        "    std::vector<float> v2 = {4, 5, 6};\n",
        "\n",
        "    printVector(v1, \"v1\");\n",
        "    printVector(v2, \"v2\");\n",
        "\n",
        "    // Addition & Subtraction\n",
        "    printVector(add(v1, v2), \"v1 + v2\");\n",
        "    printVector(subtract(v1, v2), \"v1 - v2\");\n",
        "\n",
        "    // Scalar multiplication\n",
        "    printVector(scalarMultiply(v1, 2.0f), \"2 * v1\");\n",
        "\n",
        "    // Dot product\n",
        "    std::cout << \"Dot(v1, v2) = \" << dot(v1, v2) << std::endl;\n",
        "\n",
        "    // Cross product (3D only)\n",
        "    printVector(cross(v1, v2), \"v1 x v2\");\n",
        "\n",
        "    // Norms\n",
        "    std::cout << \"L1 norm of v1 = \" << l1Norm(v1) << std::endl;\n",
        "    std::cout << \"L2 norm of v1 = \" << l2Norm(v1) << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxGus5S6ccjP",
        "outputId": "06018ac2-493f-4351-dcb7-06ed4b441de3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 02_vector_ops.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vector_ops.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcublasLt -lcublas -o vector_ops\n",
        "\n",
        "!./vector_ops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiiUTW7LdSBH",
        "outputId": "bfaa6881-330f-4f1e-9ae2-a860504b7417"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kvector_ops.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./vector_ops: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cuBlas"
      ],
      "metadata": {
        "id": "eBHyG1FdeaB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 03_vector_ops_lib.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <cublas_v2.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Helper: check CUDA errors\n",
        "#define CUDA_CHECK(x) if((x) != cudaSuccess){ \\\n",
        "    std::cerr << \"CUDA error at \" << __LINE__ << std::endl; return -1; }\n",
        "\n",
        "#define CUBLAS_CHECK(x) if((x) != CUBLAS_STATUS_SUCCESS){ \\\n",
        "    std::cerr << \"cuBLAS error at \" << __LINE__ << std::endl; return -1; }\n",
        "\n",
        "int main() {\n",
        "    // Example vectors (3D for cross product)\n",
        "    int n = 3;\n",
        "    std::vector<float> h_v1 = {1.0f, 2.0f, 3.0f};\n",
        "    std::vector<float> h_v2 = {4.0f, 5.0f, 6.0f};\n",
        "\n",
        "    float *d_v1, *d_v2;\n",
        "    CUDA_CHECK(cudaMalloc(&d_v1, n * sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_v2, n * sizeof(float)));\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(d_v1, h_v1.data(), n * sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_v2, h_v2.data(), n * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // cuBLAS handle\n",
        "    cublasHandle_t handle;\n",
        "    CUBLAS_CHECK(cublasCreate(&handle));\n",
        "\n",
        "    // --- 1. Scalar Multiplication (cublasSscal) ---\n",
        "    float alpha = 2.0f;\n",
        "    CUBLAS_CHECK(cublasSscal(handle, n, &alpha, d_v1, 1));\n",
        "    CUDA_CHECK(cudaMemcpy(h_v1.data(), d_v1, n * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    std::cout << \"2 * v1 = [ \";\n",
        "    for (auto x : h_v1) std::cout << x << \" \";\n",
        "    std::cout << \"]\" << std::endl;\n",
        "\n",
        "    // Reset v1 for other ops\n",
        "    h_v1 = {1.0f, 2.0f, 3.0f};\n",
        "    CUDA_CHECK(cudaMemcpy(d_v1, h_v1.data(), n * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // --- 2. Vector Addition/Subtraction (cublasSaxpy) ---\n",
        "    // v3 = v1 + v2\n",
        "    std::vector<float> h_result(n);\n",
        "    float *d_result;\n",
        "    CUDA_CHECK(cudaMalloc(&d_result, n * sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_result, d_v2, n * sizeof(float), cudaMemcpyDeviceToDevice)); // copy v2 → result\n",
        "    alpha = 1.0f;\n",
        "    CUBLAS_CHECK(cublasSaxpy(handle, n, &alpha, d_v1, 1, d_result, 1));\n",
        "    CUDA_CHECK(cudaMemcpy(h_result.data(), d_result, n * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    std::cout << \"v1 + v2 = [ \";\n",
        "    for (auto x : h_result) std::cout << x << \" \";\n",
        "    std::cout << \"]\" << std::endl;\n",
        "\n",
        "    // v3 = v1 - v2\n",
        "    CUDA_CHECK(cudaMemcpy(d_result, d_v1, n * sizeof(float), cudaMemcpyDeviceToDevice)); // copy v1 → result\n",
        "    alpha = -1.0f;\n",
        "    CUBLAS_CHECK(cublasSaxpy(handle, n, &alpha, d_v2, 1, d_result, 1));\n",
        "    CUDA_CHECK(cudaMemcpy(h_result.data(), d_result, n * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    std::cout << \"v1 - v2 = [ \";\n",
        "    for (auto x : h_result) std::cout << x << \" \";\n",
        "    std::cout << \"]\" << std::endl;\n",
        "\n",
        "    // --- 3. Dot Product (cublasSdot) ---\n",
        "    float dot_result;\n",
        "    CUBLAS_CHECK(cublasSdot(handle, n, d_v1, 1, d_v2, 1, &dot_result));\n",
        "    std::cout << \"Dot(v1, v2) = \" << dot_result << std::endl;\n",
        "\n",
        "    // --- 4. Cross Product (manual, not in cuBLAS) ---\n",
        "    // cuBLAS does not have cross product; implement with simple kernel\n",
        "    float h_cross[3] = {\n",
        "        h_v1[1]*h_v2[2] - h_v1[2]*h_v2[1],\n",
        "        h_v1[2]*h_v2[0] - h_v1[0]*h_v2[2],\n",
        "        h_v1[0]*h_v2[1] - h_v1[1]*h_v2[0]\n",
        "    };\n",
        "    std::cout << \"v1 x v2 = [ \" << h_cross[0] << \" \" << h_cross[1] << \" \" << h_cross[2] << \" ]\" << std::endl;\n",
        "\n",
        "    // --- 5. Norms ---\n",
        "    float norm1, norm2;\n",
        "    // L1 norm (cublasSasum)\n",
        "    CUBLAS_CHECK(cublasSasum(handle, n, d_v1, 1, &norm1));\n",
        "    std::cout << \"L1 norm of v1 = \" << norm1 << std::endl;\n",
        "\n",
        "    // L2 norm (cublasSnrm2)\n",
        "    CUBLAS_CHECK(cublasSnrm2(handle, n, d_v1, 1, &norm2));\n",
        "    std::cout << \"L2 norm of v1 = \" << norm2 << std::endl;\n",
        "\n",
        "    // Cleanup\n",
        "    cublasDestroy(handle);\n",
        "    cudaFree(d_v1);\n",
        "    cudaFree(d_v2);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17pCZ3oZdYdu",
        "outputId": "559f1cfe-8930-41cd-9da5-f924ea62ba78"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 03_vector_ops_lib.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vector_ops_lib.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcublasLt -lcublas -o vector_ops_lib\n",
        "\n",
        "!./vector_ops_lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCDg8c2TeDqx",
        "outputId": "34c6321b-be1f-45d6-9696-5c556e237dfa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kvector_ops_lib.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./vector_ops_lib: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix Operations"
      ],
      "metadata": {
        "id": "cjpaztIZfcTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 04_matrix_ops_lib.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <cusolverDn.h>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<std::endl; return -1;}\n",
        "#define CUBLAS_CHECK(x) if((x)!=CUBLAS_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuBLAS error at \"<<__LINE__<<std::endl; return -1;}\n",
        "#define CUSOLVER_CHECK(x) if((x)!=CUSOLVER_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuSOLVER error at \"<<__LINE__<<std::endl; return -1;}\n",
        "\n",
        "int main() {\n",
        "    int m=2, n=2;\n",
        "    std::vector<float> h_A = {1,2,3,4}; // row-major 2x2\n",
        "    std::vector<float> h_B = {5,6,7,8};\n",
        "    std::vector<float> h_C(m*n);\n",
        "\n",
        "    float *d_A,*d_B,*d_C;\n",
        "    CUDA_CHECK(cudaMalloc(&d_A, m*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_B, m*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_C, m*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_A,h_A.data(),m*n*sizeof(float),cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_B,h_B.data(),m*n*sizeof(float),cudaMemcpyHostToDevice));\n",
        "\n",
        "    cublasHandle_t cublasH;\n",
        "    cusolverDnHandle_t cusolverH;\n",
        "    CUBLAS_CHECK(cublasCreate(&cublasH));\n",
        "    CUSOLVER_CHECK(cusolverDnCreate(&cusolverH));\n",
        "\n",
        "    float alpha=1.0f, beta=0.0f;\n",
        "\n",
        "    // --- 1. Matrix Addition: C = A + B\n",
        "    CUBLAS_CHECK(cublasSgeam(cublasH, CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "                             m, n, &alpha, d_A, m, &alpha, d_B, m,\n",
        "                             d_C, m));\n",
        "    CUDA_CHECK(cudaMemcpy(h_C.data(), d_C, m*n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    std::cout<<\"A + B = [ \"; for(auto x:h_C) std::cout<<x<<\" \"; std::cout<<\"]\"<<std::endl;\n",
        "\n",
        "    // --- 2. Matrix Subtraction: C = A - B\n",
        "    float neg=-1.0f;\n",
        "    CUBLAS_CHECK(cublasSgeam(cublasH, CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "                             m, n, &alpha, d_A, m, &neg, d_B, m,\n",
        "                             d_C, m));\n",
        "    CUDA_CHECK(cudaMemcpy(h_C.data(), d_C, m*n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    std::cout<<\"A - B = [ \"; for(auto x:h_C) std::cout<<x<<\" \"; std::cout<<\"]\"<<std::endl;\n",
        "\n",
        "    // --- 3. Scalar Multiplication: A = 2*A\n",
        "    alpha=2.0f;\n",
        "    CUBLAS_CHECK(cublasSscal(cublasH, m*n, &alpha, d_A, 1));\n",
        "    CUDA_CHECK(cudaMemcpy(h_C.data(), d_A, m*n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    std::cout<<\"2 * A = [ \"; for(auto x:h_C) std::cout<<x<<\" \"; std::cout<<\"]\"<<std::endl;\n",
        "\n",
        "    // Reset A\n",
        "    h_A = {1,2,3,4};\n",
        "    CUDA_CHECK(cudaMemcpy(d_A,h_A.data(),m*n*sizeof(float),cudaMemcpyHostToDevice));\n",
        "\n",
        "    // --- 4. Matrix Multiplication: C = A * B\n",
        "    alpha=1.0f; beta=0.0f;\n",
        "    CUBLAS_CHECK(cublasSgemm(cublasH, CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "                             m, n, m,\n",
        "                             &alpha, d_A, m, d_B, m, &beta, d_C, m));\n",
        "    CUDA_CHECK(cudaMemcpy(h_C.data(), d_C, m*n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    std::cout<<\"A * B = [ \"; for(auto x:h_C) std::cout<<x<<\" \"; std::cout<<\"]\"<<std::endl;\n",
        "\n",
        "    // --- 5. Transpose: C = A^T\n",
        "    alpha=1.0f; beta=0.0f;\n",
        "    CUBLAS_CHECK(cublasSgeam(cublasH, CUBLAS_OP_T, CUBLAS_OP_N,\n",
        "                             m, n, &alpha, d_A, m, &beta, d_A, m, d_C, m));\n",
        "    CUDA_CHECK(cudaMemcpy(h_C.data(), d_C, m*n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    std::cout<<\"A^T = [ \"; for(auto x:h_C) std::cout<<x<<\" \"; std::cout<<\"]\"<<std::endl;\n",
        "\n",
        "    // --- 6. Identity Matrix (2x2) ---\n",
        "    std::vector<float> h_I = {1,0,0,1};\n",
        "    std::cout<<\"I = [ \"; for(auto x:h_I) std::cout<<x<<\" \"; std::cout<<\"]\"<<std::endl;\n",
        "\n",
        "    // --- 7. Inverse and Determinant with cuSOLVER ---\n",
        "    int work_size=0, *devInfo;\n",
        "    CUDA_CHECK(cudaMalloc(&devInfo, sizeof(int)));\n",
        "    float *d_inverse;\n",
        "    CUDA_CHECK(cudaMalloc(&d_inverse, m*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_inverse,h_A.data(),m*n*sizeof(float),cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Workspace size\n",
        "    CUSOLVER_CHECK(cusolverDnSgetrf_bufferSize(cusolverH,m,n,d_inverse,m,&work_size));\n",
        "    float *d_work; CUDA_CHECK(cudaMalloc(&d_work,work_size*sizeof(float)));\n",
        "\n",
        "    int *d_ipiv; CUDA_CHECK(cudaMalloc(&d_ipiv,m*sizeof(int)));\n",
        "\n",
        "    // --- LU factorization (A = L*U) ---\n",
        "    CUSOLVER_CHECK(cusolverDnSgetrf(cusolverH,m,n,d_inverse,m,d_work,d_ipiv,devInfo));\n",
        "    int h_info; CUDA_CHECK(cudaMemcpy(&h_info,devInfo,sizeof(int),cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // --- Determinant from U (product of diagonal elements) ---\n",
        "    std::vector<float> h_LU(m*n);\n",
        "    CUDA_CHECK(cudaMemcpy(h_LU.data(),d_inverse,m*n*sizeof(float),cudaMemcpyDeviceToHost));\n",
        "    float det = h_LU[0]*h_LU[3] - h_LU[1]*h_LU[2]; // valid for 2x2\n",
        "    std::cout<<\"det(A) = \"<<det<<std::endl;\n",
        "\n",
        "    // --- Inverse via solving A * X = I ---\n",
        "    float *d_I;\n",
        "    CUDA_CHECK(cudaMalloc(&d_I, m*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_I,h_I.data(),m*n*sizeof(float),cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Solve for inverse (columns of identity)\n",
        "    CUSOLVER_CHECK(cusolverDnSgetrs(cusolverH,CUBLAS_OP_N,m,n,d_inverse,m,d_ipiv,d_I,m,devInfo));\n",
        "    CUDA_CHECK(cudaMemcpy(h_C.data(),d_I,m*n*sizeof(float),cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout<<\"A^-1 = [ \"; for(auto x:h_C) std::cout<<x<<\" \"; std::cout<<\"]\"<<std::endl;\n",
        "\n",
        "    // Cleanup inverse workspace\n",
        "    cudaFree(d_I); cudaFree(d_ipiv); cudaFree(d_work); cudaFree(devInfo); cudaFree(d_inverse);\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "    cudaFree(d_inverse); cudaFree(d_ipiv); cudaFree(d_work); cudaFree(devInfo);\n",
        "    cublasDestroy(cublasH); cusolverDnDestroy(cusolverH);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdwP8RpFePZC",
        "outputId": "bdf042d9-c593-445e-f1ea-00b054723a9b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 04_matrix_ops_lib.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrix_ops_lib.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o matrix_ops_lib\n",
        "\n",
        "!./matrix_ops_lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tI0fg0KgJZA",
        "outputId": "a53281d1-4e5c-415e-d0e8-5d9c79f5a839"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kmatrix_ops_lib.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./matrix_ops_lib: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 05_norms_distance_gpu.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <cmath>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; exit(-1);}\n",
        "#define CUBLAS_CHECK(x) if((x)!=CUBLAS_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuBLAS error at \"<<__LINE__<<std::endl; exit(-1);}\n",
        "\n",
        "// CUDA kernel to compute absolute values\n",
        "__global__ void abs_kernel(const float* x, float* y, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if(idx < n) y[idx] = fabsf(x[idx]);\n",
        "}\n",
        "\n",
        "// CUDA kernel for reduction sum (simple version)\n",
        "__global__ void reduce_sum(float* data, float* result, int n) {\n",
        "    __shared__ float sdata[256];\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + tid;\n",
        "    sdata[tid] = (idx < n) ? data[idx] : 0.0f;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduce within block\n",
        "    for(int s=blockDim.x/2; s>0; s>>=1) {\n",
        "        if(tid < s) sdata[tid] += sdata[tid + s];\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write result of this block\n",
        "    if(tid == 0) atomicAdd(result, sdata[0]);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 3;\n",
        "    std::vector<float> h_x = {1.0f, -2.0f, 3.0f};\n",
        "    std::vector<float> h_y = {4.0f, 0.5f, -1.0f};\n",
        "\n",
        "    float *d_x, *d_y;\n",
        "    CUDA_CHECK(cudaMalloc(&d_x, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_y, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_x, h_x.data(), n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_y, h_y.data(), n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    cublasHandle_t handle;\n",
        "    CUBLAS_CHECK(cublasCreate(&handle));\n",
        "\n",
        "    // --- L2 Norm (Euclidean) ---\n",
        "    float l2_x, l2_y;\n",
        "    CUBLAS_CHECK(cublasSnrm2(handle, n, d_x, 1, &l2_x));\n",
        "    CUBLAS_CHECK(cublasSnrm2(handle, n, d_y, 1, &l2_y));\n",
        "    std::cout << \"L2 Norm of x = \" << l2_x << \"\\n\";\n",
        "    std::cout << \"L2 Norm of y = \" << l2_y << \"\\n\";\n",
        "\n",
        "    // --- Cosine Similarity ---\n",
        "    float dot;\n",
        "    CUBLAS_CHECK(cublasSdot(handle, n, d_x, 1, d_y, 1, &dot));\n",
        "    float cosine_sim = dot / (l2_x * l2_y);\n",
        "    std::cout << \"Cosine Similarity between x and y = \" << cosine_sim << \"\\n\";\n",
        "\n",
        "    // --- L1 Norm (Manhattan) on GPU ---\n",
        "    float *d_abs_x, *d_abs_y, *d_sum_x, *d_sum_y;\n",
        "    float h_sum_x=0.0f, h_sum_y=0.0f;\n",
        "    CUDA_CHECK(cudaMalloc(&d_abs_x, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_abs_y, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_sum_x, sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_sum_y, sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_sum_x, &h_sum_x, sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_sum_y, &h_sum_y, sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    abs_kernel<<<1, n>>>(d_x, d_abs_x, n);\n",
        "    abs_kernel<<<1, n>>>(d_y, d_abs_y, n);\n",
        "\n",
        "    reduce_sum<<<1, 256>>>(d_abs_x, d_sum_x, n);\n",
        "    reduce_sum<<<1, 256>>>(d_abs_y, d_sum_y, n);\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(&h_sum_x, d_sum_x, sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    CUDA_CHECK(cudaMemcpy(&h_sum_y, d_sum_y, sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"L1 Norm of x = \" << h_sum_x << \"\\n\";\n",
        "    std::cout << \"L1 Norm of y = \" << h_sum_y << \"\\n\";\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_x); cudaFree(d_y);\n",
        "    cudaFree(d_abs_x); cudaFree(d_abs_y);\n",
        "    cudaFree(d_sum_x); cudaFree(d_sum_y);\n",
        "    cublasDestroy(handle);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1TuQfy5wmp4",
        "outputId": "96f2fcb1-d45d-4e5b-e5e9-09c35a9ce23f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 05_norms_distance_gpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc norms_distance_gpu.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o norms_distance_gpu\n",
        "\n",
        "!./norms_distance_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIzvZFtqwohF",
        "outputId": "d19fbc2c-87e8-4211-fc6c-092bd6ede603"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Knorms_distance_gpu.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./norms_distance_gpu: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Systems of Linear Equations"
      ],
      "metadata": {
        "id": "oZ-XVHq0iuao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 06_solve_axb_cg.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; return -1;}\n",
        "#define CUBLAS_CHECK(x) if((x)!=CUBLAS_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuBLAS error at \"<<__LINE__<<std::endl; return -1;}\n",
        "\n",
        "int main() {\n",
        "    // Example SPD matrix A (3x3) in COLUMN-MAJOR order:\n",
        "    // A = [ 4 1 0\n",
        "    //       1 3 1\n",
        "    //       0 1 2 ]\n",
        "    // Column-major storage (columns concatenated):\n",
        "    std::vector<float> h_A = {\n",
        "        4.0f, 1.0f, 0.0f,   // col 0\n",
        "        1.0f, 3.0f, 1.0f,   // col 1\n",
        "        0.0f, 1.0f, 2.0f    // col 2\n",
        "    };\n",
        "    // RHS b\n",
        "    std::vector<float> h_b = {1.0f, 2.0f, 3.0f};\n",
        "\n",
        "    int n = 3; // matrix size\n",
        "\n",
        "    // Allocate device memory\n",
        "    float *d_A = nullptr, *d_x = nullptr, *d_b = nullptr;\n",
        "    float *d_r = nullptr, *d_p = nullptr, *d_Ap = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_A, n*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_x, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_b, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_r, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_p, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_Ap, n*sizeof(float)));\n",
        "\n",
        "    // Copy A and b to device\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), n*n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_b, h_b.data(), n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemset(d_x, 0, n*sizeof(float))); // initial x = 0\n",
        "\n",
        "    // cuBLAS handle\n",
        "    cublasHandle_t handle;\n",
        "    CUBLAS_CHECK(cublasCreate(&handle));\n",
        "\n",
        "    // Scalars for cuBLAS\n",
        "    const float one = 1.0f;\n",
        "    const float zero = 0.0f;\n",
        "    //const float neg_one = -1.0f;\n",
        "\n",
        "    // r0 = b - A*x0  (x0 = 0 -> r0 = b)\n",
        "    CUBLAS_CHECK(cublasScopy(handle, n, d_b, 1, d_r, 1)); // r = b\n",
        "    // p0 = r0\n",
        "    CUBLAS_CHECK(cublasScopy(handle, n, d_r, 1, d_p, 1));\n",
        "\n",
        "    // rsold = r' * r\n",
        "    float rsold;\n",
        "    CUBLAS_CHECK(cublasSdot(handle, n, d_r, 1, d_r, 1, &rsold));\n",
        "\n",
        "    const int max_iter = 1000;\n",
        "    const float tol = 1e-6f;\n",
        "    int k;\n",
        "    for (k = 0; k < max_iter; ++k) {\n",
        "        // Ap = A * p  (use cublasSgemv: y = alpha*A*x + beta*y)\n",
        "        // A is column-major, no transpose\n",
        "        CUBLAS_CHECK(cublasSgemv(handle,\n",
        "                                CUBLAS_OP_N,\n",
        "                                n,      // rows\n",
        "                                n,      // cols\n",
        "                                &one,\n",
        "                                d_A,    // A (n x n)\n",
        "                                n,      // lda\n",
        "                                d_p,    // x\n",
        "                                1,\n",
        "                                &zero,\n",
        "                                d_Ap,\n",
        "                                1));   // Ap = A * p\n",
        "\n",
        "        // alpha = rsold / (p' * Ap)\n",
        "        float pAp;\n",
        "        CUBLAS_CHECK(cublasSdot(handle, n, d_p, 1, d_Ap, 1, &pAp));\n",
        "        if (std::fabs(pAp) < 1e-12f) {\n",
        "            std::cerr << \"Break: p'Ap ~ 0 (numerical issue)\\n\";\n",
        "            break;\n",
        "        }\n",
        "        float alpha = rsold / pAp;\n",
        "\n",
        "        // x = x + alpha * p\n",
        "        CUBLAS_CHECK(cublasSaxpy(handle, n, &alpha, d_p, 1, d_x, 1));\n",
        "\n",
        "        // r = r - alpha * Ap  (use axpy with -alpha)\n",
        "        float neg_alpha = -alpha;\n",
        "        CUBLAS_CHECK(cublasSaxpy(handle, n, &neg_alpha, d_Ap, 1, d_r, 1));\n",
        "\n",
        "        // rsnew = r' * r\n",
        "        float rsnew;\n",
        "        CUBLAS_CHECK(cublasSdot(handle, n, d_r, 1, d_r, 1, &rsnew));\n",
        "\n",
        "        // Check convergence: sqrt(rsnew) = ||r||\n",
        "        float rnorm = std::sqrt(rsnew);\n",
        "        if (rnorm < tol) {\n",
        "            std::cout << \"Converged at iter \" << k+1 << \", ||r|| = \" << rnorm << \"\\n\";\n",
        "            break;\n",
        "        }\n",
        "\n",
        "        // p = r + (rsnew/rsold) * p\n",
        "        float beta = rsnew / rsold;\n",
        "        // p = beta * p\n",
        "        CUBLAS_CHECK(cublasSscal(handle, n, &beta, d_p, 1));\n",
        "        // p = p + r\n",
        "        CUBLAS_CHECK(cublasSaxpy(handle, n, &one, d_r, 1, d_p, 1));\n",
        "\n",
        "        rsold = rsnew;\n",
        "    }\n",
        "\n",
        "    // Copy solution x back to host\n",
        "    std::vector<float> h_x(n);\n",
        "    CUDA_CHECK(cudaMemcpy(h_x.data(), d_x, n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"Solution x (GPU CG): [ \";\n",
        "    for (auto v : h_x) std::cout << v << \" \";\n",
        "    std::cout << \"]\\n\";\n",
        "\n",
        "    // Compute final residual norm on host for verification: r = b - A*x\n",
        "    std::vector<float> Ax(n, 0.0f);\n",
        "    // Compute Ax on host using h_A (column-major)\n",
        "    for (int col = 0; col < n; ++col) {\n",
        "        for (int row = 0; row < n; ++row) {\n",
        "            Ax[row] += h_A[col*n + row] * h_x[col];\n",
        "        }\n",
        "    }\n",
        "    std::vector<float> h_res(n);\n",
        "    float res_norm = 0.0f;\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        h_res[i] = h_b[i] - Ax[i];\n",
        "        res_norm += h_res[i] * h_res[i];\n",
        "    }\n",
        "    res_norm = std::sqrt(res_norm);\n",
        "    std::cout << \"Residual norm ||b - Ax|| = \" << res_norm << std::endl;\n",
        "\n",
        "    // Cleanup\n",
        "    CUBLAS_CHECK(cublasDestroy(handle));\n",
        "    cudaFree(d_A); cudaFree(d_x); cudaFree(d_b);\n",
        "    cudaFree(d_r); cudaFree(d_p); cudaFree(d_Ap);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE5MGkSxgOZ0",
        "outputId": "edf1689d-5419-4d12-fc4c-9e0f70b5429a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 06_solve_axb_cg.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc solve_axb_cg.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o solve_axb_cg\n",
        "\n",
        "!./solve_axb_cg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAgCO3-mjL6k",
        "outputId": "b61b63cb-75b1-4265-cbcb-f947be4a9303"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksolve_axb_cg.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./solve_axb_cg: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cuSolver"
      ],
      "metadata": {
        "id": "uks-i959kiFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 07_solve_axb_cusolver_lib.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cusolverDn.h>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; return -1;}\n",
        "#define CUSOLVER_CHECK(x) if((x)!=CUSOLVER_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuSOLVER error at \"<<__LINE__<<std::endl; return -1;}\n",
        "\n",
        "int main() {\n",
        "    int n = 3; // matrix size\n",
        "\n",
        "    // Example matrix A (row-major)\n",
        "    std::vector<float> h_A = {4, 1, 0,\n",
        "                              1, 3, 1,\n",
        "                              0, 1, 2};\n",
        "    // Right-hand side b\n",
        "    std::vector<float> h_b = {1, 2, 3};\n",
        "\n",
        "    // Device memory\n",
        "    float *d_A = nullptr, *d_b = nullptr;\n",
        "    int *d_ipiv = nullptr, *d_info = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_A, n*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_b, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_ipiv, n*sizeof(int)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_info, sizeof(int)));\n",
        "\n",
        "    // Copy A and b to device\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), n*n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_b, h_b.data(), n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // cuSOLVER handle\n",
        "    cusolverDnHandle_t cusolverH;\n",
        "    CUSOLVER_CHECK(cusolverDnCreate(&cusolverH));\n",
        "\n",
        "    // Workspace query\n",
        "    int work_size = 0;\n",
        "    CUSOLVER_CHECK(cusolverDnSgetrf_bufferSize(cusolverH, n, n, d_A, n, &work_size));\n",
        "\n",
        "    float *d_work = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_work, work_size * sizeof(float)));\n",
        "\n",
        "    // --- LU factorization ---\n",
        "    CUSOLVER_CHECK(cusolverDnSgetrf(cusolverH, n, n, d_A, n, d_work, d_ipiv, d_info));\n",
        "\n",
        "    int h_info = 0;\n",
        "    CUDA_CHECK(cudaMemcpy(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    if (h_info != 0) {\n",
        "        std::cerr << \"LU factorization failed, info = \" << h_info << std::endl;\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // --- Solve Ax = b ---\n",
        "    CUSOLVER_CHECK(cusolverDnSgetrs(cusolverH, CUBLAS_OP_N, n, 1, d_A, n, d_ipiv, d_b, n, d_info));\n",
        "    CUDA_CHECK(cudaMemcpy(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    if (h_info != 0) {\n",
        "        std::cerr << \"Solve failed, info = \" << h_info << std::endl;\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // Copy solution back to host\n",
        "    std::vector<float> h_x(n);\n",
        "    CUDA_CHECK(cudaMemcpy(h_x.data(), d_b, n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"Solution x = [ \";\n",
        "    for (auto v : h_x) std::cout << v << \" \";\n",
        "    std::cout << \"]\\n\";\n",
        "\n",
        "    // --- Compute residual norm ||b - Ax|| ---\n",
        "    std::vector<float> res(n, 0.0f);\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        float sum = 0.0f;\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            sum += h_A[i*n + j] * h_x[j]; // row-major\n",
        "        }\n",
        "        res[i] = h_b[i] - sum;\n",
        "    }\n",
        "    float rnorm = 0.0f;\n",
        "    for (int i = 0; i < n; i++) rnorm += res[i]*res[i];\n",
        "    rnorm = std::sqrt(rnorm);\n",
        "    std::cout << \"Residual norm ||b - Ax|| = \" << rnorm << std::endl;\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_b); cudaFree(d_ipiv); cudaFree(d_info); cudaFree(d_work);\n",
        "    cusolverDnDestroy(cusolverH);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_Zd84objS76",
        "outputId": "6a8d4314-a231-40e1-ec8f-ab86c360ef79"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 07_solve_axb_cusolver_lib.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc solve_axb_cusolver_lib.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o solve_axb_cusolver_lib\n",
        "\n",
        "!./solve_axb_cusolver_lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuS__Wi0ku-7",
        "outputId": "b43d3618-473d-40f2-a97f-0b1355e7e386"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksolve_axb_cusolver_lib.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./solve_axb_cusolver_lib: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 08_olve_axb_cusolver_lib_01.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cusolverDn.h>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; return -1;}\n",
        "#define CUSOLVER_CHECK(x) if((x)!=CUSOLVER_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuSOLVER error at \"<<__LINE__<<std::endl; return -1;}\n",
        "\n",
        "int main() {\n",
        "    int n = 3; // matrix size\n",
        "\n",
        "    // Example matrix A (row-major)\n",
        "    std::vector<float> h_A = {4, 1, 0,\n",
        "                              1, 3, 1,\n",
        "                              0, 1, 2};\n",
        "    // Right-hand side b\n",
        "    std::vector<float> h_b = {1, 2, 3};\n",
        "\n",
        "    // Device memory\n",
        "    float *d_A = nullptr, *d_b = nullptr;\n",
        "    int *d_ipiv = nullptr, *d_info = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_A, n*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_b, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_ipiv, n*sizeof(int)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_info, sizeof(int)));\n",
        "\n",
        "    // Copy A and b to device\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), n*n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_b, h_b.data(), n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // cuSOLVER handle\n",
        "    cusolverDnHandle_t cusolverH;\n",
        "    CUSOLVER_CHECK(cusolverDnCreate(&cusolverH));\n",
        "\n",
        "    // Workspace query\n",
        "    int work_size = 0;\n",
        "    CUSOLVER_CHECK(cusolverDnSgetrf_bufferSize(cusolverH, n, n, d_A, n, &work_size));\n",
        "\n",
        "    float *d_work = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_work, work_size * sizeof(float)));\n",
        "\n",
        "    // --- LU factorization ---\n",
        "    CUSOLVER_CHECK(cusolverDnSgetrf(cusolverH, n, n, d_A, n, d_work, d_ipiv, d_info));\n",
        "\n",
        "    int h_info = 0;\n",
        "    CUDA_CHECK(cudaMemcpy(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    if (h_info != 0) {\n",
        "        std::cerr << \"LU factorization failed, info = \" << h_info << std::endl;\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // --- Solve Ax = b ---\n",
        "    CUSOLVER_CHECK(cusolverDnSgetrs(cusolverH, CUBLAS_OP_N, n, 1, d_A, n, d_ipiv, d_b, n, d_info));\n",
        "    CUDA_CHECK(cudaMemcpy(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    if (h_info != 0) {\n",
        "        std::cerr << \"Solve failed, info = \" << h_info << std::endl;\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // Copy solution back to host\n",
        "    std::vector<float> h_x(n);\n",
        "    CUDA_CHECK(cudaMemcpy(h_x.data(), d_b, n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"Solution x = [ \";\n",
        "    for (auto v : h_x) std::cout << v << \" \";\n",
        "    std::cout << \"]\\n\";\n",
        "\n",
        "    // --- Compute residual norm ||b - Ax|| in double precision ---\n",
        "    std::vector<double> h_xd(n), h_Ad(n*n), h_bd(n);\n",
        "    for (int i=0;i<n;i++) h_xd[i] = static_cast<double>(h_x[i]);\n",
        "    for (int i=0;i<n*n;i++) h_Ad[i] = static_cast<double>(h_A[i]);\n",
        "    for (int i=0;i<n;i++) h_bd[i] = static_cast<double>(h_b[i]);\n",
        "\n",
        "    std::vector<double> res(n,0.0);\n",
        "    for (int i=0;i<n;i++) {\n",
        "        double sum=0.0;\n",
        "        for (int j=0;j<n;j++) sum += h_Ad[i*n + j]*h_xd[j];\n",
        "        res[i] = h_bd[i] - sum;\n",
        "    }\n",
        "    double rnorm=0.0;\n",
        "    for (int i=0;i<n;i++) rnorm += res[i]*res[i];\n",
        "    rnorm = std::sqrt(rnorm);\n",
        "\n",
        "    std::cout << \"Residual norm ||b - Ax|| = \" << rnorm << std::endl;\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_b); cudaFree(d_ipiv); cudaFree(d_info); cudaFree(d_work);\n",
        "    cusolverDnDestroy(cusolverH);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTsisiuxk5Fu",
        "outputId": "8922bbaa-a6cb-4672-9b39-136e73e04b7f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 08_olve_axb_cusolver_lib_01.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc solve_axb_cusolver_lib_01.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o solve_axb_cusolver_lib_01\n",
        "\n",
        "!./solve_axb_cusolver_lib_01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBEq4-hYl9l8",
        "outputId": "4a2af112-0fa3-4564-c7e9-3d6869bddb9c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksolve_axb_cusolver_lib_01.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./solve_axb_cusolver_lib_01: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Transformations and Neural Network"
      ],
      "metadata": {
        "id": "TKsDVSfLmydK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 09_linear_transform_nn_cuda.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; return -1;}\n",
        "#define CUBLAS_CHECK(x) if((x)!=CUBLAS_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuBLAS error at \"<<__LINE__<<std::endl; return -1;}\n",
        "\n",
        "// ReLU activation on host\n",
        "void relu(std::vector<float> &v){\n",
        "    for(auto &x:v) if(x<0) x=0;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // 3D points: 3 x 3 (each column = a point)\n",
        "    std::vector<float> h_points = {1, 0, 1,\n",
        "                                   0, 1, 1,\n",
        "                                   0, 0, 1}; // 3 points\n",
        "    int num_points = 3;\n",
        "\n",
        "    // 3x3 rotation around Z by 90 deg + scaling\n",
        "    float theta = 90.0f;\n",
        "    float rad = theta * M_PI / 180.0f;\n",
        "    float scale = 2.0f;\n",
        "    std::vector<float> h_R = {\n",
        "        scale * cos(rad), -scale * sin(rad), 0,\n",
        "        scale * sin(rad),  scale * cos(rad), 0,\n",
        "        0,                0,                scale\n",
        "    };\n",
        "\n",
        "    // Device memory\n",
        "    float *d_points, *d_R, *d_result;\n",
        "    CUDA_CHECK(cudaMalloc(&d_points, 3*num_points*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_R, 3*3*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_result, 3*num_points*sizeof(float)));\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(d_points, h_points.data(), 3*num_points*sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_R, h_R.data(), 3*3*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    cublasHandle_t handle;\n",
        "    CUBLAS_CHECK(cublasCreate(&handle));\n",
        "\n",
        "    float alpha = 1.0f, beta = 0.0f;\n",
        "    // Linear transformation: result = R * points\n",
        "    CUBLAS_CHECK(cublasSgemm(handle,\n",
        "                             CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "                             3, num_points, 3,\n",
        "                             &alpha,\n",
        "                             d_R, 3,\n",
        "                             d_points, 3,\n",
        "                             &beta,\n",
        "                             d_result, 3));\n",
        "\n",
        "    // Copy result back\n",
        "    std::vector<float> h_result(3*num_points);\n",
        "    CUDA_CHECK(cudaMemcpy(h_result.data(), d_result, 3*num_points*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"Original 3D points:\\n\";\n",
        "    for(int i=0;i<num_points;i++) std::cout << \"(\" << h_points[3*i] << \",\" << h_points[3*i+1] << \",\" << h_points[3*i+2] << \")\\n\";\n",
        "\n",
        "    std::cout << \"Transformed 3D points (rotation + scaling):\\n\";\n",
        "    for(int i=0;i<num_points;i++) std::cout << \"(\" << h_result[3*i] << \",\" << h_result[3*i+1] << \",\" << h_result[3*i+2] << \")\\n\";\n",
        "\n",
        "    // ---------------- Simple NN Forward ----------------\n",
        "    // Input: 3 features x 3 samples\n",
        "    // Layer1: 3x3 weight + bias\n",
        "    std::vector<float> W1 = {0.5, -0.2, 0.1,\n",
        "                             0.3, 0.8, -0.5,\n",
        "                             -0.6, 0.1, 0.4};\n",
        "    std::vector<float> b1 = {0.1, -0.1, 0.2};\n",
        "\n",
        "    float *d_W1, *d_b1;\n",
        "    CUDA_CHECK(cudaMalloc(&d_W1, 3*3*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_b1, 3*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_W1, W1.data(), 3*3*sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_b1, b1.data(), 3*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Layer1: Y = W1*X + b\n",
        "    CUBLAS_CHECK(cublasSgemm(handle,\n",
        "                             CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "                             3, num_points, 3,\n",
        "                             &alpha,\n",
        "                             d_W1, 3,\n",
        "                             d_result, 3,\n",
        "                             &beta,\n",
        "                             d_points, 3)); // reuse d_points for output\n",
        "\n",
        "    // Copy result back for activation\n",
        "    CUDA_CHECK(cudaMemcpy(h_result.data(), d_points, 3*num_points*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // Add bias and ReLU\n",
        "    for(int i=0;i<num_points;i++)\n",
        "        for(int j=0;j<3;j++)\n",
        "            h_result[j + 3*i] += b1[j];\n",
        "    relu(h_result);\n",
        "\n",
        "    std::cout << \"NN Layer1 output (after ReLU):\\n\";\n",
        "    for(int i=0;i<num_points;i++) std::cout << \"(\" << h_result[3*i] << \",\" << h_result[3*i+1] << \",\" << h_result[3*i+2] << \")\\n\";\n",
        "\n",
        "    // Cleanup\n",
        "    cublasDestroy(handle);\n",
        "    cudaFree(d_points); cudaFree(d_R); cudaFree(d_result);\n",
        "    cudaFree(d_W1); cudaFree(d_b1);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A0pyH5Cl-Qs",
        "outputId": "15894123-0162-4b22-c52f-e20aedc22e42"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 09_linear_transform_nn_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc linear_transform_nn_cuda.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o linear_transform_nn_cuda\n",
        "\n",
        "!./linear_transform_nn_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF0aDODHm_y7",
        "outputId": "2b9b6f3c-b98a-4bd2-d7fe-19d9ee533a7a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Klinear_transform_nn_cuda.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./linear_transform_nn_cuda: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvalues and Eigenvectors"
      ],
      "metadata": {
        "id": "8ZIK3b9Rnd9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 10_eigen_cuda_lib.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cusolverDn.h>\n",
        "#include <cmath>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; return -1;}\n",
        "#define CUSOLVER_CHECK(x) if((x)!=CUSOLVER_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuSOLVER error at \"<<__LINE__<<std::endl; return -1;}\n",
        "\n",
        "int main() {\n",
        "    // Symmetric matrix 3x3\n",
        "    std::vector<float> h_A = {4, 1, 1,\n",
        "                              1, 3, 0,\n",
        "                              1, 0, 2}; // row-major\n",
        "    int n = 3;\n",
        "\n",
        "    // Device memory\n",
        "    float *d_A;\n",
        "    CUDA_CHECK(cudaMalloc(&d_A, n*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), n*n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // cuSOLVER handle\n",
        "    cusolverDnHandle_t solverH;\n",
        "    CUSOLVER_CHECK(cusolverDnCreate(&solverH));\n",
        "\n",
        "    // Workspace\n",
        "    int lwork = 0;\n",
        "    CUSOLVER_CHECK(cusolverDnSsyevd_bufferSize(solverH, CUSOLVER_EIG_MODE_VECTOR, CUBLAS_FILL_MODE_UPPER, n, d_A, n, nullptr, &lwork));\n",
        "\n",
        "    float *d_W, *d_work;\n",
        "    int *devInfo;\n",
        "    CUDA_CHECK(cudaMalloc(&d_W, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_work, lwork*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&devInfo, sizeof(int)));\n",
        "\n",
        "    // Compute eigenvalues and eigenvectors\n",
        "    CUSOLVER_CHECK(cusolverDnSsyevd(solverH, CUSOLVER_EIG_MODE_VECTOR, CUBLAS_FILL_MODE_UPPER, n, d_A, n, d_W, d_work, lwork, devInfo));\n",
        "\n",
        "    int h_info;\n",
        "    CUDA_CHECK(cudaMemcpy(&h_info, devInfo, sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    if (h_info != 0) {\n",
        "        std::cerr << \"Eigen decomposition failed, info = \" << h_info << std::endl;\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // Copy results to host\n",
        "    std::vector<float> h_W(n);\n",
        "    std::vector<float> h_V(n*n);\n",
        "    CUDA_CHECK(cudaMemcpy(h_W.data(), d_W, n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    CUDA_CHECK(cudaMemcpy(h_V.data(), d_A, n*n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"Eigenvalues:\\n\";\n",
        "    for(auto w:h_W) std::cout << w << \" \";\n",
        "    std::cout << \"\\nEigenvectors (columns):\\n\";\n",
        "    for(int i=0;i<n;i++){\n",
        "        for(int j=0;j<n;j++) std::cout << h_V[j + i*n] << \" \";\n",
        "        std::cout << \"\\n\";\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_W); cudaFree(d_work); cudaFree(devInfo);\n",
        "    cusolverDnDestroy(solverH);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9QDgXAinGzy",
        "outputId": "723e5c75-988d-48e1-d1a6-4ffab48c8c5e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 10_eigen_cuda_lib.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc eigen_cuda_lib.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o eigen_cuda_lib\n",
        "\n",
        "!./eigen_cuda_lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rjib0KndnjRT",
        "outputId": "7a73da0c-0087-4c51-a79a-30797587b8f9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Keigen_cuda_lib.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./eigen_cuda_lib: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decomposition"
      ],
      "metadata": {
        "id": "nEdNRb69omma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 11_qr_cuda_lib.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cusolverDn.h>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; return -1;}\n",
        "#define CUSOLVER_CHECK(x) if((x)!=CUSOLVER_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuSOLVER error at \"<<__LINE__<<std::endl; return -1;}\n",
        "\n",
        "int main() {\n",
        "    int m=3, n=2; // matrix size\n",
        "    std::vector<float> h_A = {12, -51,\n",
        "                               6, 167,\n",
        "                              -4, 24}; // row-major\n",
        "\n",
        "    float *d_A; CUDA_CHECK(cudaMalloc(&d_A, m*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), m*n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    cusolverDnHandle_t solverH;\n",
        "    CUSOLVER_CHECK(cusolverDnCreate(&solverH));\n",
        "\n",
        "    int lwork=0;\n",
        "    CUSOLVER_CHECK(cusolverDnSgeqrf_bufferSize(solverH, m, n, d_A, m, &lwork));\n",
        "\n",
        "    float *d_tau, *d_work; int *devInfo;\n",
        "    CUDA_CHECK(cudaMalloc(&d_tau, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_work, lwork*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&devInfo, sizeof(int)));\n",
        "\n",
        "    // Compute QR factorization\n",
        "    CUSOLVER_CHECK(cusolverDnSgeqrf(solverH, m, n, d_A, m, d_tau, d_work, lwork, devInfo));\n",
        "\n",
        "    // Copy back results\n",
        "    std::vector<float> h_R(m*n);\n",
        "    std::vector<float> h_tau(n);\n",
        "    CUDA_CHECK(cudaMemcpy(h_R.data(), d_A, m*n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    CUDA_CHECK(cudaMemcpy(h_tau.data(), d_tau, n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"R (upper-triangular part of A):\\n\";\n",
        "    for(int i=0;i<m;i++){\n",
        "        for(int j=0;j<n;j++) std::cout<<h_R[i + j*m]<<\" \";\n",
        "        std::cout<<std::endl;\n",
        "    }\n",
        "\n",
        "    cudaFree(d_A); cudaFree(d_tau); cudaFree(d_work); cudaFree(devInfo);\n",
        "    cusolverDnDestroy(solverH);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jCezjxOnrpr",
        "outputId": "e7cd3eb3-0589-45c0-e2e6-5b62662a0eef"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 11_qr_cuda_lib.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc qr_cuda_lib.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o qr_cuda_lib\n",
        "\n",
        "!./qr_cuda_lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMa1RzXXotsU",
        "outputId": "15f52a33-b550-46f5-db56-2d41ad521f5f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kqr_cuda_lib.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./qr_cuda_lib: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 12_svd_cuda_lib.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cusolverDn.h>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; return -1;}\n",
        "#define CUSOLVER_CHECK(x) if((x)!=CUSOLVER_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuSOLVER error at \"<<__LINE__<<std::endl; return -1;}\n",
        "\n",
        "int main() {\n",
        "    int m=3, n=2;\n",
        "    std::vector<float> h_A = {1, 0,\n",
        "                              0, 1,\n",
        "                              1, 1}; // row-major m x n\n",
        "\n",
        "    float *d_A;\n",
        "    CUDA_CHECK(cudaMalloc(&d_A, m*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), m*n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    cusolverDnHandle_t solverH;\n",
        "    CUSOLVER_CHECK(cusolverDnCreate(&solverH));\n",
        "\n",
        "    int lwork=0;\n",
        "    CUSOLVER_CHECK(cusolverDnSgesvd_bufferSize(solverH, m, n, &lwork));\n",
        "\n",
        "    float *d_S, *d_U, *d_VT, *d_work; int *devInfo;\n",
        "    CUDA_CHECK(cudaMalloc(&d_S, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_U, m*m*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_VT, n*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_work, lwork*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&devInfo, sizeof(int)));\n",
        "\n",
        "    signed char jobu = 'A', jobvt = 'A';\n",
        "    CUSOLVER_CHECK(cusolverDnSgesvd(solverH, jobu, jobvt, m, n, d_A, m, d_S, d_U, m, d_VT, n, d_work, lwork, nullptr, devInfo));\n",
        "\n",
        "    std::vector<float> h_S(n);\n",
        "    CUDA_CHECK(cudaMemcpy(h_S.data(), d_S, n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"Singular values:\\n\";\n",
        "    for(auto s:h_S) std::cout << s << \" \";\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    cudaFree(d_A); cudaFree(d_S); cudaFree(d_U); cudaFree(d_VT); cudaFree(d_work); cudaFree(devInfo);\n",
        "    cusolverDnDestroy(solverH);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YsKA45OozYL",
        "outputId": "7ab00a50-e10c-48e4-8147-c80c09a8141f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 12_svd_cuda_lib.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc svd_cuda_lib.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o svd_cuda_lib\n",
        "\n",
        "!./svd_cuda_lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7mTXQSbo_RM",
        "outputId": "8b9c6b15-fde6-4251-923a-988b3c44b9a4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksvd_cuda_lib.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./svd_cuda_lib: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Orthogonality"
      ],
      "metadata": {
        "id": "tF1VOrp1pYkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 13_orth_proj_cuda.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; return -1;}\n",
        "#define CUBLAS_CHECK(x) if((x)!=CUBLAS_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuBLAS error at \"<<__LINE__<<std::endl; return -1;}\n",
        "\n",
        "// Gram-Schmidt orthonormalization on host\n",
        "void gram_schmidt(std::vector<float>& v1, std::vector<float>& v2) {\n",
        "    // Normalize v1\n",
        "    float norm1 = std::sqrt(v1[0]*v1[0]+v1[1]*v1[1]+v1[2]*v1[2]);\n",
        "    for(int i=0;i<3;i++) v1[i] /= norm1;\n",
        "\n",
        "    // v2 = v2 - proj_v1(v2)\n",
        "    float dot = v1[0]*v2[0] + v1[1]*v2[1] + v1[2]*v2[2];\n",
        "    for(int i=0;i<3;i++) v2[i] -= dot*v1[i];\n",
        "\n",
        "    // Normalize v2\n",
        "    float norm2 = std::sqrt(v2[0]*v2[0]+v2[1]*v2[1]+v2[2]*v2[2]);\n",
        "    for(int i=0;i<3;i++) v2[i] /= norm2;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // 3D vectors spanning subspace\n",
        "    std::vector<float> v1 = {1,1,0};\n",
        "    std::vector<float> v2 = {1,0,1};\n",
        "    gram_schmidt(v1,v2);\n",
        "\n",
        "    // Vector to project\n",
        "    std::vector<float> b = {3,2,1};\n",
        "\n",
        "    // Device memory\n",
        "    float *d_Q, *d_b, *d_proj;\n",
        "    CUDA_CHECK(cudaMalloc(&d_Q, 3*2*sizeof(float))); // 2 orthonormal vectors\n",
        "    CUDA_CHECK(cudaMalloc(&d_b, 3*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_proj, 3*sizeof(float)));\n",
        "\n",
        "    // Copy Q and b\n",
        "    std::vector<float> h_Q = {v1[0], v1[1], v1[2],\n",
        "                              v2[0], v2[1], v2[2]}; // column-major\n",
        "    CUDA_CHECK(cudaMemcpy(d_Q, h_Q.data(), 3*2*sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_b, b.data(), 3*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    cublasHandle_t handle;\n",
        "    CUBLAS_CHECK(cublasCreate(&handle));\n",
        "\n",
        "    float alpha=1.0f, beta=0.0f;\n",
        "    // proj = Q * (Q^T * b)\n",
        "    float *d_temp;\n",
        "    CUDA_CHECK(cudaMalloc(&d_temp, 2*sizeof(float))); // 2x1 vector\n",
        "    // d_temp = Q^T * b\n",
        "    CUBLAS_CHECK(cublasSgemv(handle, CUBLAS_OP_T, 3, 2, &alpha, d_Q, 3, d_b, 1, &beta, d_temp, 1));\n",
        "    // d_proj = Q * d_temp\n",
        "    CUBLAS_CHECK(cublasSgemv(handle, CUBLAS_OP_N, 3, 2, &alpha, d_Q, 3, d_temp, 1, &beta, d_proj, 1));\n",
        "\n",
        "    // Copy projection back\n",
        "    std::vector<float> h_proj(3);\n",
        "    CUDA_CHECK(cudaMemcpy(h_proj.data(), d_proj, 3*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"Projection of b onto subspace spanned by v1 and v2:\\n\";\n",
        "    std::cout << \"(\" << h_proj[0] << \", \" << h_proj[1] << \", \" << h_proj[2] << \")\\n\";\n",
        "\n",
        "    cudaFree(d_Q); cudaFree(d_b); cudaFree(d_proj); cudaFree(d_temp);\n",
        "    cublasDestroy(handle);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2oW-4qIpGIa",
        "outputId": "97a51fe4-453c-4dfc-e8fd-5c5f35dd304e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 13_orth_proj_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc orth_proj_cuda.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o orth_proj_cuda\n",
        "\n",
        "!./orth_proj_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFxsvM6hppRD",
        "outputId": "5dacfaee-88d5-4080-fe19-d2d6127a4c94"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Korth_proj_cuda.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./orth_proj_cuda: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 14_orth_proj_lib.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cusolverDn.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; return -1;}\n",
        "#define CUSOLVER_CHECK(x) if((x)!=CUSOLVER_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuSOLVER error at \"<<__LINE__<<std::endl; return -1;}\n",
        "#define CUBLAS_CHECK(x) if((x)!=CUBLAS_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuBLAS error at \"<<__LINE__<<std::endl; return -1;}\n",
        "\n",
        "int main() {\n",
        "    int m = 3; // dimension\n",
        "    int n = 2; // number of vectors spanning subspace\n",
        "\n",
        "    // Columns are vectors spanning subspace\n",
        "    std::vector<float> h_A = {1,1,0,  // v1\n",
        "                              1,0,1}; // v2\n",
        "    // Row-major to column-major\n",
        "    std::vector<float> h_A_col(m*n);\n",
        "    for(int j=0;j<n;j++)\n",
        "        for(int i=0;i<m;i++)\n",
        "            h_A_col[i + j*m] = h_A[j*m + i];\n",
        "\n",
        "    // Vector to project\n",
        "    std::vector<float> h_b = {3,2,1};\n",
        "\n",
        "    float *d_A, *d_tau, *d_b, *d_proj;\n",
        "    CUDA_CHECK(cudaMalloc(&d_A, m*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_tau, n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_b, m*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_proj, m*sizeof(float)));\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, h_A_col.data(), m*n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_b, h_b.data(), m*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    cusolverDnHandle_t solverH;\n",
        "    CUSOLVER_CHECK(cusolverDnCreate(&solverH));\n",
        "\n",
        "    int lwork = 0;\n",
        "    CUSOLVER_CHECK(cusolverDnSgeqrf_bufferSize(solverH, m, n, d_A, m, &lwork));\n",
        "\n",
        "    float *d_work;\n",
        "    int *devInfo;\n",
        "    CUDA_CHECK(cudaMalloc(&d_work, lwork*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&devInfo, sizeof(int)));\n",
        "\n",
        "    // Compute QR decomposition: d_A -> Q (orthonormal vectors in first n columns)\n",
        "    CUSOLVER_CHECK(cusolverDnSgeqrf(solverH, m, n, d_A, m, d_tau, d_work, lwork, devInfo));\n",
        "\n",
        "    // Generate explicit Q\n",
        "    CUSOLVER_CHECK(cusolverDnSorgqr(solverH, m, n, n, d_A, m, d_tau, d_work, lwork, devInfo));\n",
        "\n",
        "    // Now d_A contains Q (m x n), orthonormal columns\n",
        "    cublasHandle_t handle;\n",
        "    CUBLAS_CHECK(cublasCreate(&handle));\n",
        "\n",
        "    float alpha=1.0f, beta=0.0f;\n",
        "    float *d_temp;\n",
        "    CUDA_CHECK(cudaMalloc(&d_temp, n*sizeof(float)));\n",
        "\n",
        "    // temp = Q^T * b\n",
        "    CUBLAS_CHECK(cublasSgemv(handle, CUBLAS_OP_T, m, n, &alpha, d_A, m, d_b, 1, &beta, d_temp, 1));\n",
        "    // proj = Q * temp\n",
        "    CUBLAS_CHECK(cublasSgemv(handle, CUBLAS_OP_N, m, n, &alpha, d_A, m, d_temp, 1, &beta, d_proj, 1));\n",
        "\n",
        "    // Copy projection back\n",
        "    std::vector<float> h_proj(m);\n",
        "    CUDA_CHECK(cudaMemcpy(h_proj.data(), d_proj, m*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"Projection of b onto subspace spanned by Q:\\n\";\n",
        "    for(int i=0;i<m;i++) std::cout << h_proj[i] << \" \";\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_tau); cudaFree(d_b); cudaFree(d_proj); cudaFree(d_work); cudaFree(d_temp); cudaFree(devInfo);\n",
        "    cusolverDnDestroy(solverH);\n",
        "    cublasDestroy(handle);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViVnfUSHpsg5",
        "outputId": "df0b28ff-8703-4e54-cedb-9852b702349a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 14_orth_proj_lib.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc orth_proj_lib.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o orth_proj_lib\n",
        "\n",
        "!./orth_proj_lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d24gnIE3p3dI",
        "outputId": "1a6ae062-0266-4fb9-9f73-d2eb75134086"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Korth_proj_lib.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./orth_proj_lib: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 15_rank_nullspace_cublas.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; return -1;}\n",
        "#define CUBLAS_CHECK(x) if((x)!=CUBLAS_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuBLAS error at \"<<__LINE__<<std::endl; return -1;}\n",
        "\n",
        "int main() {\n",
        "    // Matrix dimensions\n",
        "    int m = 2, n = 3;\n",
        "\n",
        "    // Column-major matrix A\n",
        "    std::vector<float> h_A = {\n",
        "        1,4,   // col 0\n",
        "        2,5,   // col 1\n",
        "        3,6    // col 2\n",
        "    };\n",
        "\n",
        "    // Device memory\n",
        "    float *d_A, *d_AtA;\n",
        "    CUDA_CHECK(cudaMalloc(&d_A, m*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), m*n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMalloc(&d_AtA, n*n*sizeof(float)));\n",
        "\n",
        "    // cuBLAS handle\n",
        "    cublasHandle_t handle;\n",
        "    CUBLAS_CHECK(cublasCreate(&handle));\n",
        "\n",
        "    float alpha = 1.0f, beta = 0.0f;\n",
        "\n",
        "    // Compute AtA = A^T * A\n",
        "    // A: m x n, AtA: n x n\n",
        "    CUBLAS_CHECK(cublasSgemm(handle, CUBLAS_OP_T, CUBLAS_OP_N,\n",
        "                             n, n, m, &alpha,\n",
        "                             d_A, m,\n",
        "                             d_A, m,\n",
        "                             &beta, d_AtA, n));\n",
        "\n",
        "    // Copy AtA to host\n",
        "    std::vector<float> h_AtA(n*n);\n",
        "    CUDA_CHECK(cudaMemcpy(h_AtA.data(), d_AtA, n*n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"A^T * A = \\n\";\n",
        "    for(int i=0;i<n;i++){\n",
        "        for(int j=0;j<n;j++) std::cout << h_AtA[i + j*n] << \" \"; // column-major\n",
        "        std::cout << \"\\n\";\n",
        "    }\n",
        "\n",
        "    // Approximate rank by counting non-zero diagonals\n",
        "    float eps = 1e-6;\n",
        "    int rank = 0;\n",
        "    for(int i=0;i<n;i++) if(fabs(h_AtA[i + i*n]) > eps) rank++;\n",
        "    std::cout << \"Approximate Rank of A = \" << rank << std::endl;\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_AtA);\n",
        "    cublasDestroy(handle);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDZafnRyp-KX",
        "outputId": "1982d6bc-dafc-4ec6-d696-589faa828fcd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 15_rank_nullspace_cublas.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc rank_nullspace_cublas.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o rank_nullspace_cublas\n",
        "\n",
        "!./rank_nullspace_cublas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHcgiOX3qaNG",
        "outputId": "4580f440-4645-4426-9d98-e45de522cf82"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Krank_nullspace_cublas.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./rank_nullspace_cublas: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 16_nullspace_cublas.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <cmath>\n",
        "\n",
        "#define CUDA_CHECK(x) if((x)!=cudaSuccess){ \\\n",
        "    std::cerr<<\"CUDA error at \"<<__LINE__<<\": \"<<cudaGetErrorString(x)<<std::endl; exit(-1);}\n",
        "#define CUBLAS_CHECK(x) if((x)!=CUBLAS_STATUS_SUCCESS){ \\\n",
        "    std::cerr<<\"cuBLAS error at \"<<__LINE__<<std::endl; exit(-1);}\n",
        "\n",
        "// Compute Euclidean norm of a vector on GPU\n",
        "float gpu_norm(cublasHandle_t handle, float* d_x, int len) {\n",
        "    float result;\n",
        "    CUBLAS_CHECK(cublasSnrm2(handle, len, d_x, 1, &result));\n",
        "    return result;\n",
        "}\n",
        "\n",
        "// Subtract projection: x = x - (v^T x) * v\n",
        "void gpu_subtract_projection(cublasHandle_t handle, float* d_x, float* d_v, int len) {\n",
        "    float dot;\n",
        "    CUBLAS_CHECK(cublasSdot(handle, len, d_v, 1, d_x, 1, &dot));\n",
        "    float alpha = -dot;\n",
        "    CUBLAS_CHECK(cublasSaxpy(handle, len, &alpha, d_v, 1, d_x, 1));\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int m = 2, n = 3;\n",
        "    std::vector<float> h_A = {\n",
        "        1,4,  // col 0\n",
        "        2,5,  // col 1\n",
        "        3,6   // col 2\n",
        "    };\n",
        "\n",
        "    float *d_A;\n",
        "    CUDA_CHECK(cudaMalloc(&d_A, m*n*sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), m*n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    cublasHandle_t handle;\n",
        "    CUBLAS_CHECK(cublasCreate(&handle));\n",
        "\n",
        "    // Null space vector\n",
        "    float *d_x;\n",
        "    CUDA_CHECK(cudaMalloc(&d_x, n*sizeof(float)));\n",
        "\n",
        "    // Initial guess (non-zero vector)\n",
        "    std::vector<float> h_x = {1.0f, -2.0f, 1.0f};\n",
        "    CUDA_CHECK(cudaMemcpy(d_x, h_x.data(), n*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Orthogonalize w.r.t columns of A (Gram-Schmidt)\n",
        "    for(int j=0; j<m; j++) {\n",
        "        float *d_col = d_A + j; // column-major stride\n",
        "        gpu_subtract_projection(handle, d_x, d_col, n);\n",
        "    }\n",
        "\n",
        "    // Normalize\n",
        "    float norm = gpu_norm(handle, d_x, n);\n",
        "    float alpha = 1.0f/norm;\n",
        "    CUBLAS_CHECK(cublasSscal(handle, n, &alpha, d_x, 1));\n",
        "\n",
        "    // Copy back to host\n",
        "    CUDA_CHECK(cudaMemcpy(h_x.data(), d_x, n*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    std::cout << \"Null space basis vector (Ax=0): [ \";\n",
        "    for(auto v : h_x) std::cout << v << \" \";\n",
        "    std::cout << \"]\\n\";\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_x);\n",
        "    cublasDestroy(handle);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-0RAsBwqk8d",
        "outputId": "d9d827d7-eabe-4940-b9c6-4b6e76b41025"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 16_nullspace_cublas.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc nullspace_cublas.cu -I/usr/include -L/usr/lib/x86_64-linux-gnu -lcutensor -lcusolver -lcublasLt -lcublas -o nullspace_cublas\n",
        "\n",
        "!./nullspace_cublas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuWwPHrjvTgC",
        "outputId": "484cad6c-2afa-447a-c184-fe4f6a7493bb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Knullspace_cublas.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./nullspace_cublas: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir cu_c++_code"
      ],
      "metadata": {
        "id": "3dlUC3S9zBHS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp *.cu cu_c++_code/"
      ],
      "metadata": {
        "id": "ceQdluqGzFW1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r cu_c++_code.zip cu_c++_code/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr_31Jv5zMzW",
        "outputId": "2b02e0e8-92d3-43b7-9d27-81ea7cee6574"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: cu_c++_code/ (stored 0%)\n",
            "  adding: cu_c++_code/04_matrix_ops_lib.cu (deflated 76%)\n",
            "  adding: cu_c++_code/02_vector_ops.cu (deflated 72%)\n",
            "  adding: cu_c++_code/01_cutensor_basics.cu (deflated 63%)\n",
            "  adding: cu_c++_code/03_vector_ops_lib.cu (deflated 72%)\n",
            "  adding: cu_c++_code/00_test_cutensor.cu (deflated 43%)\n",
            "  adding: cu_c++_code/12_svd_cuda_lib.cu (deflated 62%)\n",
            "  adding: cu_c++_code/09_linear_transform_nn_cuda.cu (deflated 72%)\n",
            "  adding: cu_c++_code/15_rank_nullspace_cublas.cu (deflated 59%)\n",
            "  adding: cu_c++_code/07_solve_axb_cusolver_lib.cu (deflated 67%)\n",
            "  adding: cu_c++_code/05_norms_distance_gpu.cu (deflated 67%)\n",
            "  adding: cu_c++_code/13_orth_proj_cuda.cu (deflated 62%)\n",
            "  adding: cu_c++_code/11_qr_cuda_lib.cu (deflated 61%)\n",
            "  adding: cu_c++_code/08_olve_axb_cusolver_lib_01.cu (deflated 67%)\n",
            "  adding: cu_c++_code/10_eigen_cuda_lib.cu (deflated 63%)\n",
            "  adding: cu_c++_code/06_solve_axb_cg.cu (deflated 68%)\n",
            "  adding: cu_c++_code/16_nullspace_cublas.cu (deflated 60%)\n",
            "  adding: cu_c++_code/14_orth_proj_lib.cu (deflated 66%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cu_c++_code.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0o5xzZJsx6dU",
        "outputId": "90c85916-1996-4764-ecd4-14be506575f3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_630820a1-9172-410c-9ec1-2ed6a8eb49b2\", \"cu_c++_code.zip\", 19214)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8wVPnRbOy8xh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}